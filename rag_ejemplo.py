# -*- coding: utf-8 -*-
"""Ejemplo de Sistema RAG con LangChain

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dharcN8iZSlDCNiPs92YE00-OH_eV3XH
"""

# -*- coding: utf-8 -*-
# Este script demuestra un sistema RAG (Generación Aumentada por Recuperación) simple
# utilizando LangChain para un caso de estudio de atención al cliente.

# Importar las librerías necesarias
from langchain_community.llms import Ollama
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain_community.document_loaders import TextLoader
import os

# Paso 1: Definir la base de conocimiento de la empresa (documentos).
# En un caso real, esto serían archivos PDF, TXT, bases de datos, etc.
# Aquí usamos una cadena de texto simple para el ejercicio.

# Documento de ejemplo con información de EcoMarket
DOCUMENTO_ECO_MARKET = """
Política de Devoluciones de EcoMarket:

Nuestra política de devoluciones permite a los clientes devolver productos dentro de los 30 días posteriores a la fecha de compra. Para ser elegible para una devolución, el artículo debe estar sin usar, en su embalaje original y en las mismas condiciones en que lo recibió. Los productos perecederos, como alimentos y flores, no son elegibles para devolución. Las devoluciones de ropa de segunda mano también están sujetas a una revisión estricta para garantizar que el artículo no haya sido usado después de la compra. Para iniciar una devolución, visite nuestra sección de ayuda en el sitio web y siga los pasos.
"""

# Paso 2: Crear el modelo de embeddings.
# Este modelo convierte el texto de los documentos en vectores.
# Usamos un modelo de Ollama para el ejemplo, que se ejecuta localmente.
# NOTA: Asegúrate de tener Ollama instalado y un modelo como `llama2` descargado (`ollama pull llama2`)
try:
    embeddings = OllamaEmbeddings(model="llama2")
except Exception as e:
    print(f"Error al inicializar el modelo de embeddings de Ollama: {e}")
    print("Asegúrate de tener Ollama corriendo en tu máquina y el modelo 'llama2' descargado.")
    exit()

# Paso 3: Dividir el documento en fragmentos (chunks) para una búsqueda eficiente.
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
docs = text_splitter.create_documents([DOCUMENTO_ECO_MARKET])

# Paso 4: Crear la base de datos vectorial en memoria y cargar los fragmentos.
# Usamos ChromaDB, que es fácil de configurar y funciona en memoria para este ejemplo.
print("Creando y cargando la base de datos vectorial...")
vectorstore = Chroma.from_documents(documents=docs, embedding=embeddings)

# Paso 5: Configurar el modelo de lenguaje (LLM).
# Usamos un modelo de Ollama que se ejecuta localmente.
# Esto asegura que el proceso se puede ejecutar sin claves API externas.
try:
    llm = Ollama(model="llama2")
except Exception as e:
    print(f"Error al inicializar el modelo LLM de Ollama: {e}")
    print("Asegúrate de tener Ollama corriendo en tu máquina y el modelo 'llama2' descargado.")
    exit()

# Paso 6: Crear el "chain" de RAG.
# Un `chain` une el recuperador de documentos y el LLM.
# El "retriever" buscará los documentos más relevantes.
# Luego, el "LLM" generará la respuesta usando esos documentos como contexto.
retriever = vectorstore.as_retriever()
qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever)

# Paso 7: Interactuar con el sistema RAG.
print("\n¡El sistema RAG para EcoMarket está listo!")
print("Escribe una pregunta y presiona Enter. Escribe 'salir' para terminar.")

while True:
    pregunta = input("\nTu pregunta: ")
    if pregunta.lower() == 'salir':
        break

    # Realizar la consulta
    print("Buscando y generando respuesta...")
    respuesta = qa_chain.run(pregunta)

    print("\nRespuesta del Asistente:\n")
    print(respuesta)